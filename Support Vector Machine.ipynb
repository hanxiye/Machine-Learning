{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "This is from Homework 3 of EECS 545 in Winter 2016.\n",
    "\n",
    "Recall that maximizing the soft margin in SVM is equivalent to the following minimization problem:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\min_{{\\bf w}, b} & & \\frac{1}{2} \\Vert{\\bf w}\\Vert^2 + C \\sum_{i=1}^N \\xi_i \\\\\n",
    "& \\text{subject to} & & t^{(i)} ( {\\bf w}^T {\\bf x}^{(i)} + b ) \\ge 1 - \\xi_i\\\\\n",
    "& & & \\xi_i \\ge 0 \\qquad (i = 1, \\ldots, N)\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Equivalently, we can solve the following unconstrained minimization problem:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "& \\min_{{\\bf w}, b} & & \\frac{1}{2} \\Vert{\\bf w}\\Vert^2 + C \\sum_{i=1}^N \\max\\left(0, 1 - t^{(i)} ( {\\bf w}^T {\\bf x}^{(i)} + b ) \\right)\n",
    "\\end{aligned}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Prove that minimization problem (1) and (2) are equivalent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    " <font color='blue'>Proof: We know that if any $\\xi_i^*$ is optimal, one of the conditions $t^{(i)}({\\bf w}^T {\\bf x}^{(i)} + b)  = 1 - \\xi_i^*$ and $\\xi_i^* = 0$ must hold. \n",
    "Therefore, we can plug $\\xi_i^* = \\max(0, 1 - t^{(i)} ( {\\bf w}^T {\\bf x}^{(i)} + b ) $ and get the unconstrained minimization problem.</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Let $(\\mathbf{w}^*, b^*, \\boldsymbol{\\xi}^*)$ be the solution of minimization problem (1). Show that if $\\xi_i^*>0$, then the distance from the training data point $\\mathbf{x}^{(i)}$ to the margin hyperplane $t^{(i)}((\\mathbf{w}^*)^T\\mathbf{x}+b^*)=1$ is proportional to $\\xi^*_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='blue'>Proof: We know from (a) that if $\\xi_i^*>0$, we must have $\\xi_i^* = 1 - t^{(i)}({\\bf w}^T {\\bf x}^{(i)} + b) > 0.$\n",
    "The distance from $\\mathbf{x}^{(i)}$ to the margin hyperplane is simply\n",
    "\\begin{equation}\n",
    "|r| = \\frac{|t^{(i)}((\\mathbf{w}^*)^T\\mathbf{x}+b^*)-1|}{\\|w\\|} = \\frac{1-t^{(i)}((\\mathbf{w}^*)^T\\mathbf{x}+b^*)}{\\|\\mathbf{w}\\|} = \\frac{\\xi_i^*}{\\|\\mathbf{w}\\|}\n",
    "\\end{equation}</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(c) The error function in minimization problem (2) is\n",
    "\t$$E({\\bf w}, b) =  \\frac{1}{2} \\Vert{\\bf w}\\Vert^2 + C \\sum_{i=1}^N \\max\\left(0, 1 - t^{(i)} ( {\\bf w}^T {\\bf x}^{(i)} + b ) \\right)$$\n",
    "\tFind its derivatives: $\\nabla_{\\bf w} E({\\bf w}, b)$ and $\\frac{\\partial}{\\partial b} E({\\bf w}, b)$. Where the derivative is undefined, use a subderivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='blue'>Taking derivative with respect to ${\\bf w}$\n",
    "\\begin{equation}\n",
    "\\nabla_{\\bf w} E({\\bf w}, b) = {\\bf w} + C\\sum_{i=1}^N \\bf{1}_{\\{t^{(i)}({\\bf w}^\\top x^{(i)}+b) < 1 \\}} \\cdot (-t^{(i)}x^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "Taking derivative with respect to $b$,\n",
    "\\begin{equation}\n",
    "\\frac{\\partial}{\\partial b} E({\\bf w}, b) =  C\\sum_{i=1}^N \\bf{1}_{\\{t^{(i)}({\\bf w}^\\top x^{(i)}+b) < 1 \\}} \\cdot (-t^{(i)})\n",
    "\\end{equation}</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Implement the soft-margin SVM using batch gradient descent.  \n",
    "The learning rate for the $j$-th iteration is defined as:   \n",
    "    \\begin{equation}\n",
    "        \\alpha(j) = \\frac{\\eta_0}{1 + j \\cdot \\eta_0}    \n",
    "    \\end{equation}   \n",
    "Set $\\eta_0$ to $0.001$ and the slack cost $C$ to $3$. Show the iteration-versus-accuracy (training accuracy) plot. The training and test data/labels are provided in [digits\\_training\\_data.csv], [digits\\_training\\_labels.csv], [digits\\_test\\_data.csv] and [digits\\_test\\_labels.csv]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
